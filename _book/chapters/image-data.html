<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ron Mahabir">

<title>3&nbsp; Satellite Imagery Data – Book of Imagery</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/01_use-case-x.html" rel="next">
<link href="../chapters/intro.html" rel="prev">
<link href="../assets/Imago-logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/image-data.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Satellite Imagery Data</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Book of Imagery</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/Imago-SDRUK/boi" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preface</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction: Why Imagery, Why Now?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/image-data.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Satellite Imagery Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01_use-case-x.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Use Case 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/synthesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Synthesis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/way-forward.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Way Forward</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">3.1</span> Introduction</a></li>
  <li><a href="#chapter-objectives" id="toc-chapter-objectives" class="nav-link" data-scroll-target="#chapter-objectives"><span class="header-section-number">3.2</span> Chapter Objectives</a></li>
  <li><a href="#fundamentals-of-satellite-imagery" id="toc-fundamentals-of-satellite-imagery" class="nav-link" data-scroll-target="#fundamentals-of-satellite-imagery"><span class="header-section-number">3.3</span> Fundamentals of Satellite Imagery</a>
  <ul class="collapse">
  <li><a href="#sensor-types" id="toc-sensor-types" class="nav-link" data-scroll-target="#sensor-types"><span class="header-section-number">3.3.1</span> Sensor Types</a></li>
  <li><a href="#resolution-dimensions" id="toc-resolution-dimensions" class="nav-link" data-scroll-target="#resolution-dimensions"><span class="header-section-number">3.3.2</span> Resolution Dimensions</a></li>
  <li><a href="#key-satellite-platforms-and-missions" id="toc-key-satellite-platforms-and-missions" class="nav-link" data-scroll-target="#key-satellite-platforms-and-missions"><span class="header-section-number">3.3.3</span> Key Satellite Platforms and Missions</a></li>
  </ul></li>
  <li><a href="#data-acquisition-and-accessibility" id="toc-data-acquisition-and-accessibility" class="nav-link" data-scroll-target="#data-acquisition-and-accessibility"><span class="header-section-number">3.4</span> Data Acquisition and Accessibility</a>
  <ul class="collapse">
  <li><a href="#sources-of-satellite-imagery" id="toc-sources-of-satellite-imagery" class="nav-link" data-scroll-target="#sources-of-satellite-imagery"><span class="header-section-number">3.4.1</span> Sources of Satellite Imagery</a></li>
  <li><a href="#access-platforms-and-apis" id="toc-access-platforms-and-apis" class="nav-link" data-scroll-target="#access-platforms-and-apis"><span class="header-section-number">3.4.2</span> Access Platforms and APIs</a></li>
  <li><a href="#licensing-cost-and-ethics" id="toc-licensing-cost-and-ethics" class="nav-link" data-scroll-target="#licensing-cost-and-ethics"><span class="header-section-number">3.4.3</span> Licensing, Cost, and Ethics</a></li>
  <li><a href="#cloud-based-repositories-and-big-data-challenges" id="toc-cloud-based-repositories-and-big-data-challenges" class="nav-link" data-scroll-target="#cloud-based-repositories-and-big-data-challenges"><span class="header-section-number">3.4.4</span> Cloud-Based Repositories and Big Data Challenges</a></li>
  </ul></li>
  <li><a href="#pre-processing-and-calibration" id="toc-pre-processing-and-calibration" class="nav-link" data-scroll-target="#pre-processing-and-calibration"><span class="header-section-number">3.5</span> Pre-processing and Calibration</a></li>
  <li><a href="#georeferencing-and-orthorectification" id="toc-georeferencing-and-orthorectification" class="nav-link" data-scroll-target="#georeferencing-and-orthorectification"><span class="header-section-number">3.6</span> Georeferencing and Orthorectification</a>
  <ul class="collapse">
  <li><a href="#radiometric-and-atmospheric-corrections" id="toc-radiometric-and-atmospheric-corrections" class="nav-link" data-scroll-target="#radiometric-and-atmospheric-corrections"><span class="header-section-number">3.6.1</span> Radiometric and Atmospheric Corrections</a></li>
  <li><a href="#cloud-masking-and-data-fusion" id="toc-cloud-masking-and-data-fusion" class="nav-link" data-scroll-target="#cloud-masking-and-data-fusion"><span class="header-section-number">3.6.2</span> Cloud Masking and Data Fusion</a></li>
  <li><a href="#handling-noise-and-inconsistencies" id="toc-handling-noise-and-inconsistencies" class="nav-link" data-scroll-target="#handling-noise-and-inconsistencies"><span class="header-section-number">3.6.3</span> Handling Noise and Inconsistencies</a></li>
  <li><a href="#derivation-of-indices" id="toc-derivation-of-indices" class="nav-link" data-scroll-target="#derivation-of-indices"><span class="header-section-number">3.6.4</span> Derivation of Indices</a></li>
  </ul></li>
  <li><a href="#analytical-methods-and-tools" id="toc-analytical-methods-and-tools" class="nav-link" data-scroll-target="#analytical-methods-and-tools"><span class="header-section-number">3.7</span> Analytical Methods and Tools</a>
  <ul class="collapse">
  <li><a href="#image-classification" id="toc-image-classification" class="nav-link" data-scroll-target="#image-classification"><span class="header-section-number">3.7.1</span> Image Classification</a></li>
  <li><a href="#change-detection" id="toc-change-detection" class="nav-link" data-scroll-target="#change-detection"><span class="header-section-number">3.7.2</span> Change Detection</a></li>
  <li><a href="#object-based-image-analysis" id="toc-object-based-image-analysis" class="nav-link" data-scroll-target="#object-based-image-analysis"><span class="header-section-number">3.7.3</span> Object-Based Image Analysis</a></li>
  <li><a href="#time-series-analysis-and-spatio-temporal-modelling" id="toc-time-series-analysis-and-spatio-temporal-modelling" class="nav-link" data-scroll-target="#time-series-analysis-and-spatio-temporal-modelling"><span class="header-section-number">3.7.4</span> Time-Series Analysis and Spatio-Temporal Modelling</a></li>
  <li><a href="#software-and-programming-tools" id="toc-software-and-programming-tools" class="nav-link" data-scroll-target="#software-and-programming-tools"><span class="header-section-number">3.7.5</span> Software and Programming Tools</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">3.8</span> Summary</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/Imago-SDRUK/boi/edit/main/chapters/image-data.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/Imago-SDRUK/boi/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Satellite Imagery Data</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ron Mahabir </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">3.1</span> Introduction</h2>
<p>Satellite imagery constitutes one of the most powerful sources of information for understanding the Earth’s surface and its dynamic processes. Defined broadly as grid-based or raster data, where the Earth’s surface is represented as a matrix of pixels (or “grid cells”), each with a value corresponding to a specific measurement, captured by sensors mounted on orbital platforms. These data encompass a range of spectral, spatial, temporal, and radiometric characteristics that distinguish them from traditional, ground-based environmental observations. Unlike airborne systems that typically conduct surveys using aircraft such as small to medium-sized planes, helicopters, or drones (UAVs), or <em>in situ</em> field measurements, satellite data provide consistent, synoptic coverage across national and continental scales, enabling systematic monitoring over time.</p>
<p>Since the launch of Sputnik 1<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> in 1957 and Landsat 1<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> in 1972, satellite remote sensing has rapidly advanced alongside improvements in sensor technology, data storage, and transmission capabilities. Today, hundreds of satellites operated by governments and private companies provide images with a level of detail and frequency that would have been impossible just a decade ago. This chapter introduces the main types of satellite imagery, how they are collected and prepared for analysis, and common methods used to interpret them.</p>
</section>
<section id="chapter-objectives" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="chapter-objectives"><span class="header-section-number">3.2</span> Chapter Objectives</h2>
<ol type="1">
<li>Introduce key concepts and terminology associated with satellite imagery.<br>
</li>
<li>Situate satellite data within the wider geospatial data ecosystem, comparing it with traditional data collection methods.<br>
</li>
<li>Describe the main sensor types, platforms, and missions, highlighting their strengths and constraints.<br>
</li>
<li>Explain acquisition pathways, preprocessing workflows, and common analytical approaches.<br>
</li>
<li>Highlight the value of satellite imagery for environmental monitoring, urban planning, and disaster response, emphasising its role in supporting informed decision-making at multiple scales.</li>
</ol>
</section>
<section id="fundamentals-of-satellite-imagery" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="fundamentals-of-satellite-imagery"><span class="header-section-number">3.3</span> Fundamentals of Satellite Imagery</h2>
<section id="sensor-types" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="sensor-types"><span class="header-section-number">3.3.1</span> Sensor Types</h3>
<p>Satellite sensors are tools that collect data about the Earth by detecting different types of electromagnetic energy. These sensors vary mainly in the portion of the electromagnetic spectrum they measure and the kind of data they produce. The electromagnetic spectrum includes all forms of light, from visible colours to wavelengths the human eye cannot see, such as infrared and microwave radiation <span class="citation" data-cites="campbell2011introduction">(<a href="references.html#ref-campbell2011introduction" role="doc-biblioref">Campbell and Wynne 2011</a>)</span>. Sensors fall into two main categories: passive sensors, which measure natural energy (usually sunlight) reflected or emitted from the Earth’s surface, and active sensors, which emit their own signal and measure how it reflects back <span class="citation" data-cites="jensen2009remote">(<a href="references.html#ref-jensen2009remote" role="doc-biblioref">Jensen 2009</a>)</span>. Understanding these differences is important because each type of sensor offers specific advantages depending on the observation needs.</p>
<p>Optical sensors capture reflected sunlight in visible and near-infrared wavelengths, creating images similar to photographs. These are commonly used for land cover mapping and vegetation analysis <span class="citation" data-cites="campbell2011introduction">(<a href="references.html#ref-campbell2011introduction" role="doc-biblioref">Campbell and Wynne 2011</a>)</span>. Radar sensors, especially Synthetic Aperture Radar (SAR), send out microwave signals and measure the reflected response, allowing them to collect data in all weather conditions and at night <span class="citation" data-cites="ferretti2002permanent">(<a href="references.html#ref-ferretti2002permanent" role="doc-biblioref">Ferretti, Prati, and Rocca 2002</a>)</span>. Thermal sensors detect heat emitted from the Earth’s surface and are often used to monitor surface temperatures, detect wildfires, and assess building heat loss <span class="citation" data-cites="jensen2009remote">(<a href="references.html#ref-jensen2009remote" role="doc-biblioref">Jensen 2009</a>)</span>. Hyperspectral sensors record information across hundreds of narrow image bands (i.e., electromagnetic wavelengths), making it possible to detect subtle differences in surface materials, which is valuable in areas such as agriculture, environmental monitoring, and mineral exploration <span class="citation" data-cites="richards2022remote goetz2009three">(<a href="references.html#ref-richards2022remote" role="doc-biblioref">Richards, Richards, et al. 2022</a>; <a href="references.html#ref-goetz2009three" role="doc-biblioref">Goetz 2009</a>)</span>.</p>
<p>A summary of these sensor types is provided in <a href="#tbl-sensor_types" class="quarto-xref">Table&nbsp;<span>3.1</span></a>. Each sensor type is suited to specific applications. Optical imagery is effective for monitoring crops, forests, and urban development. Radar is ideal in areas with frequent cloud cover or during night-time, for instance in flood mapping or infrastructure monitoring. Thermal imagery supports early wildfire detection and energy audits of buildings. Hyperspectral data enable detailed analysis of surface materials, supporting targeted agricultural practices and environmental assessments.</p>
<p>These sensors provide essential information for decision-making in areas such as disaster response, climate monitoring, land management, and infrastructure planning.</p>
<div id="tbl-sensor_types" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sensor_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Caption text
</figcaption>
<div aria-describedby="tbl-sensor_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Attribute</strong></th>
<th><strong>Optical</strong> (e.g.&nbsp;Landsat 8 OLI)</th>
<th><strong>Radar</strong> (e.g.&nbsp;Sentinel-1 SAR)</th>
<th><strong>Thermal</strong> (e.g.&nbsp;MODIS, ECOSTRESS)</th>
<th><strong>Hyperspectral</strong> (e.g.&nbsp;EO-1 Hyperion)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Energy Source</strong></td>
<td>Passive</td>
<td>Active</td>
<td>Passive</td>
<td>Passive</td>
</tr>
<tr class="even">
<td><strong>Wavelength Range</strong></td>
<td>Visible and near-infrared</td>
<td>Microwave</td>
<td>Thermal infrared</td>
<td>Hundreds of narrow spectral bands</td>
</tr>
<tr class="odd">
<td><strong>Key Capabilities</strong></td>
<td>Captures sunlight reflected from the Earth’s surface to produce imagery comparable to photographs.</td>
<td>Transmits microwave pulses and measures the reflected signal to detect surface features and movement.</td>
<td>Measures heat naturally emitted from the Earth’s surface, providing information on temperature variations.</td>
<td>Records continuous spectral data across numerous narrow bands, enabling identification of surface materials.</td>
</tr>
<tr class="even">
<td><strong>Common Applications</strong></td>
<td>Land cover classification, vegetation health monitoring, urban growth analysis.</td>
<td>Flood mapping, ground deformation studies, forest structure analysis.</td>
<td>Wildfire detection, urban heat island assessment, thermal efficiency studies.</td>
<td>Precision agriculture, mineral mapping, environmental quality assessments.</td>
</tr>
<tr class="odd">
<td><strong>Strengths</strong></td>
<td>High spatial resolution and easily interpretable images.</td>
<td>Weather- and light-independent, consistent data acquisition.</td>
<td>Effective for identifying temperature anomalies and thermal patterns.</td>
<td>Fine-grained detection of subtle spectral differences among materials.</td>
</tr>
<tr class="even">
<td><strong>Limitations</strong></td>
<td>Affected by cloud cover and requires daylight.</td>
<td>Complex to interpret, needs specialised processing.</td>
<td>Lower spatial resolution and less visual detail than optical sensors.</td>
<td>Large data volumes, sensitive to atmospheric conditions.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>: Comparison of satellite sensor types.</p>
</section>
<section id="resolution-dimensions" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="resolution-dimensions"><span class="header-section-number">3.3.2</span> Resolution Dimensions</h3>
<p>Understanding the concept of resolution is critical when working with satellite imagery. Each type of resolution describes a different aspect of how satellite data captures and represents features on the Earth’s surface. Together, they determine the usefulness of the imagery for particular applications. The four main types of resolution are spatial, temporal, spectral, and radiometric resolution. A summary of these differences is presented in <a href="#tbl-resolution_comparison" class="quarto-xref">Table&nbsp;<span>3.2</span></a> along with a visual depiction of their main differences in <a href="#fig-imagery_resolution_types" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>.</p>
<div id="tbl-resolution_comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-resolution_comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Comparison of the four key resolution dimensions in satellite imagery.
</figcaption>
<div aria-describedby="tbl-resolution_comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 24%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Resolution Type</strong></th>
<th><strong>Definition</strong></th>
<th><strong>Example Satellites / Sensors</strong></th>
<th><strong>Typical Applications</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Spatial Resolution</td>
<td>Size of ground area represented by each pixel.</td>
<td>WorldView-3 (0.31 m), Sentinel-2 (10 m), MODIS (250–1000 m)</td>
<td>Urban planning, infrastructure monitoring, land cover classification</td>
</tr>
<tr class="even">
<td>Temporal Resolution</td>
<td>Frequency with which a satellite revisits the same location.</td>
<td>PlanetScope (daily), Sentinel-2 (5 days with constellation), Landsat 8 (16 days)</td>
<td>Change detection, crop monitoring, disaster response</td>
</tr>
<tr class="odd">
<td>Spectral Resolution</td>
<td>Number and width of spectral bands captured.</td>
<td>Sentinel-2 (13 bands), Hyperion (220 bands)</td>
<td>Vegetation health, mineral mapping, environmental analysis</td>
</tr>
<tr class="even">
<td>Radiometric Resolution</td>
<td>Sensor’s sensitivity to differences in reflectance or brightness.</td>
<td>Landsat 8 (12-bit), MODIS (12-bit)</td>
<td>Vegetation stress detection, water quality, surface temperature analysis</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-imagery_resolution_types" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-imagery_resolution_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/satellite_imagery resolution_types.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-imagery_resolution_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Satellite imagery resolution types.
</figcaption>
</figure>
</div>
<hr>
<p>Spatial resolution, one of the most commonly considered indicators of imagery usefulness, refers to the ground area represented by a single pixel in a satellite image. It determines the level of detail visible in the data. High spatial resolution, such as that offered by WorldView-3 (0.31 m) <span class="citation" data-cites="WorldView">(<a href="references.html#ref-WorldView" role="doc-biblioref">n.d.a</a>)</span>, enables the identification of fine-scale features like individual vehicles or small buildings (see example in <a href="#fig-spatial_resolution" class="quarto-xref">Figure&nbsp;<span>3.2</span></a>). In contrast, moderate to low spatial resolution sensors such as MODIS (250–1000 m) are better suited to observing broader phenomena like regional vegetation patterns or land cover changes.</p>
<div id="fig-spatial_resolution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-spatial_resolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/spatial_resolution_seagrasses.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-spatial_resolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Comparison of different remote sensing images from data sources for mapping seagrass habitats, illustrating how spatial resolution varies across platforms. From left to right: drone imagery, WorldView-2 Panchromatic, WorldView-2 Multispectral, PlanetScope, Sentinel-2, and Landsat 8. Each panel highlights the trade-offs between spatial resolution, cost, spectral/temporal coverage, and availability, with advantages shown in green and limitations in red. Source: Levi Westerveld, GRID-Arendal (<a href="https://www.grida.no/resources/14322">link</a>).
</figcaption>
</figure>
</div>
<hr>
<p>Temporal resolution defines how frequently a satellite captures imagery of the same geographic location. This is essential for observing changes over time, particularly in dynamic or rapidly evolving environments. Satellites with high temporal resolution, such as PlanetScope, can image locations on a near-daily basis, which is valuable for monitoring crop growth, flood events, or wildfire spread. Others, such as Landsat 8, revisit the same location every 16 days, making them more appropriate for long-term environmental monitoring <span class="citation" data-cites="wulder2012opening">(<a href="references.html#ref-wulder2012opening" role="doc-biblioref">Wulder et al. 2012</a>)</span>.</p>
<p>Spectral resolution is the ability of a sensor to detect and differentiate between various wavelengths of electromagnetic radiation. It is determined by the number of spectral bands and their widths. Multispectral sensors like Sentinel-2, which captures data in 13 bands, are suitable for general environmental monitoring. Hyperspectral sensors such as Hyperion, which collects data in 220 narrow bands, allow for fine discrimination of materials and are used in applications such as vegetation stress detection, mineral mapping, and pollution monitoring <span class="citation" data-cites="goetz2009three">(<a href="references.html#ref-goetz2009three" role="doc-biblioref">Goetz 2009</a>)</span>.</p>
<p>Radiometric resolution refers to the sensitivity of a sensor in detecting slight differences in the intensity of radiation energy or brightness. It is expressed in bits, where higher values indicate a greater capacity to capture subtle variations in reflectance. For instance, an 8-bit sensor can record 256 levels of intensity, while a 12-bit sensor, like that on Landsat 8, can distinguish 4,096 levels. Higher radiometric resolution is especially useful in detecting nuanced surface conditions such as vegetation health or surface temperature gradients <span class="citation" data-cites="roy2014landsat">(<a href="references.html#ref-roy2014landsat" role="doc-biblioref">Roy et al. 2014</a>)</span>.</p>
<p>Each resolution type plays a unique role in how satellite imagery can be interpreted and applied. For instance, high spatial resolution is vital for mapping urban features, while high spectral resolution is critical for distinguishing vegetation types or detecting subtle land changes. Most practical applications require balancing these types of resolution according to user needs and available data sources.</p>
</section>
<section id="key-satellite-platforms-and-missions" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="key-satellite-platforms-and-missions"><span class="header-section-number">3.3.3</span> Key Satellite Platforms and Missions</h3>
<p>Government-funded satellite missions play a crucial role in providing foundational Earth observation datasets. These datasets are typically made freely accessible to the public, making them invaluable resources for academic research, policy-making, and humanitarian efforts such as disaster relief and environmental monitoring. One of the most important examples is the Landsat program, which has been continuously capturing imagery since 1972 <span class="citation" data-cites="USGS">(<a href="references.html#ref-USGS" role="doc-biblioref">n.d.b</a>)</span>. This long-term archive provides a unique temporal record that allows researchers to study changes in land use, deforestation, urban expansion, and other landscape dynamics over decades <span class="citation" data-cites="wulder2019current">(<a href="references.html#ref-wulder2019current" role="doc-biblioref">Wulder et al. 2019</a>)</span>.</p>
<p>Building on this, the Sentinel missions under the European Union’s Copernicus programme offer enhanced capabilities. Launched since 2014, the Sentinel satellites provide higher spatial and spectral resolution and more frequent revisit times than earlier systems, improving the ability to monitor rapid environmental changes. For example, Sentinel-1 uses Synthetic Aperture Radar (SAR) to capture images regardless of cloud cover or daylight, which is critical for monitoring floods or infrastructure <span class="citation" data-cites="drusch2012sentinel ESA">(<a href="references.html#ref-drusch2012sentinel" role="doc-biblioref">Drusch et al. 2012</a>; <a href="references.html#ref-ESA" role="doc-biblioref">n.d.c</a>)</span>.</p>
<p>Complementing these public missions, commercial satellite platforms have emerged, offering very high resolution (i.e., sub-metre) imagery often updated daily or more frequently. Companies like Maxar Technologies provide detailed images that can resolve objects such as individual vehicles or small buildings, enabling applications in infrastructure monitoring, urban planning, and disaster response where rapid, detailed information is essential.</p>
<p>More recently, constellations of small satellites operated by companies such as Planet Labs have transformed Earth observation by offering near-daily global coverage at fine spatial resolutions of 3.7 metres. These small satellites, or “smallsats” (also called Doves), balance spatial resolution and temporal frequency, making it possible to track dynamic changes like crop growth, flooding, or urban development with unprecedented detail and frequency. However, this high temporal resolution often comes with higher costs and data management challenges <span class="citation" data-cites="curzi2020large Planet">(<a href="references.html#ref-curzi2020large" role="doc-biblioref">Curzi, Modenini, and Tortora 2020</a>; <a href="references.html#ref-Planet" role="doc-biblioref">n.d.d</a>)</span>.</p>
<p>Together, these government and commercial systems provide a powerful and complementary range of satellite imagery options, supporting a wide variety of scientific, policy, and commercial applications. Live locations of many of these satellites can be tracked online via the <a href="https://satellitemap.space/">satellitemap.space online platform</a>.</p>
</section>
</section>
<section id="data-acquisition-and-accessibility" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="data-acquisition-and-accessibility"><span class="header-section-number">3.4</span> Data Acquisition and Accessibility</h2>
<section id="sources-of-satellite-imagery" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="sources-of-satellite-imagery"><span class="header-section-number">3.4.1</span> Sources of Satellite Imagery</h3>
<p>Satellite imagery is obtainable from open-access government missions and commercial providers. Public missions such as United States Geological Survey (USGS) Landsat programme and the European Space Agency’s (ESA) Sentinel satellites (part of the Copernicus programme) have played a crucial role in making Earth observation data widely accessible. These missions offer free, standardised, and globally consistent imagery that supports long-term environmental monitoring, disaster response, land use classification, and climate change research <span class="citation" data-cites="wulder2012opening drusch2012sentinel">(<a href="references.html#ref-wulder2012opening" role="doc-biblioref">Wulder et al. 2012</a>; <a href="references.html#ref-drusch2012sentinel" role="doc-biblioref">Drusch et al. 2012</a>)</span>. The Landsat archive, in particular, provides the longest continuous record of Earth’s surface from space, dating back to 1972 <span class="citation" data-cites="wulder2019current roy2014landsat">(<a href="references.html#ref-wulder2019current" role="doc-biblioref">Wulder et al. 2019</a>; <a href="references.html#ref-roy2014landsat" role="doc-biblioref">Roy et al. 2014</a>)</span>.</p>
<p>In contrast, commercial providers such as Maxar Technologies and Planet Labs deliver very high-resolution imagery with more frequent updates. These images, which can capture detail as fine as 30 cm, are well-suited to applications such as infrastructure monitoring, precision agriculture, and emergency management <span class="citation" data-cites="belward2015launched">(<a href="references.html#ref-belward2015launched" role="doc-biblioref">Belward and Skøien 2015</a>)</span>. However, this level of detail often comes at a cost. In this case, data from commercial providers is typically subject to strict licensing and usage fees, which can limit availability for academic or humanitarian purposes.</p>
<p>To bridge this gap, hybrid access models are emerging. Initiatives like NASA’s Commercial Smallsat Data Acquisition (CSDA) programme <span class="citation" data-cites="csda">(<a href="references.html#ref-csda" role="doc-biblioref">n.d.e</a>)</span> and ESA’s Third Party Missions programme <span class="citation" data-cites="esa_third_party">(<a href="references.html#ref-esa_third_party" role="doc-biblioref">n.d.f</a>)</span> allow researchers and non-profits to access commercial satellite imagery under subsidised agreements, expanding the reach of high-resolution data for scientific and public-good applications.</p>
</section>
<section id="access-platforms-and-apis" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="access-platforms-and-apis"><span class="header-section-number">3.4.2</span> Access Platforms and APIs</h3>
<p>Cloud-based platforms and application programming interfaces (APIs) have significantly transformed how users access and analyse satellite data. Cloud-based platforms are online environments that store large datasets and provide tools for processing them remotely, removing the need for users to download large files or maintain powerful local computers. APIs are software tools that allow users to interact with these platforms programmatically, enabling them to automate tasks such as searching for images, retrieving data, and running analyses.</p>
<p>One such platform is Google Earth Engine (GEE), which combines an extensive archive of satellite imagery, including Landsat, Sentinel, and MODIS, with powerful cloud computing tools that allow users to process data at global scale without needing to download large files or maintain local servers <span class="citation" data-cites="gorelick2017google">(<a href="references.html#ref-gorelick2017google" role="doc-biblioref">Gorelick et al. 2017</a>)</span>. GEE provides a user-friendly JavaScript and Python API that enables both interactive exploration and batch processing of satellite data. This has opened up Earth observation to a broader community, including researchers, practitioners, and students who may not have access to high-performance computing infrastructure. Through GEE’s platform, users can perform complex analyses such as land cover classification, deforestation tracking, and climate monitoring across large spatial and temporal extents. The combination of open data, cloud-based processing, and accessible programming tools makes GEE a foundational resource in the modern remote sensing landscape <span class="citation" data-cites="zhao2021progress mutanga2019google">(<a href="references.html#ref-zhao2021progress" role="doc-biblioref">Zhao et al. 2021</a>; <a href="references.html#ref-mutanga2019google" role="doc-biblioref">Mutanga and Kumar 2019</a>)</span>.</p>
<p>Other platforms like the Copernicus Open Access Hub <span class="citation" data-cites="Copernicus">(<a href="references.html#ref-Copernicus" role="doc-biblioref">n.d.g</a>)</span> and USGS EarthExplorer <span class="citation" data-cites="EarthExplorer">(<a href="references.html#ref-EarthExplorer" role="doc-biblioref">Survey, n.d.</a>)</span> provide direct access to raw imagery and metadata from Sentinel and Landsat satellites. These portals support browsing, visual inspection, and batch downloads, which are particularly useful for researchers. Further, commercial providers such as Planet and Maxar also offer APIs that allow users to search, request, and download imagery programmatically. Some APIs even support satellite tasking, allowing users to request a new image over a specific location. These tools enable integration into automated workflows, making satellite data more usable in machine learning models, urban monitoring systems, and near-real-time environmental applications.</p>
</section>
<section id="licensing-cost-and-ethics" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="licensing-cost-and-ethics"><span class="header-section-number">3.4.3</span> Licensing, Cost, and Ethics</h3>
<p>Licensing models vary considerably and determine how satellite imagery can be used, shared, or modified. A licence is a legal agreement that outlines what users are allowed to do with a dataset. Open-access datasets, such as those provided by the Landsat and Sentinel missions, typically come with licences that permit free download, use, and redistribution <span class="citation" data-cites="wulder2012opening harris2015open">(<a href="references.html#ref-wulder2012opening" role="doc-biblioref">Wulder et al. 2012</a>; <a href="references.html#ref-harris2015open" role="doc-biblioref">Harris and Baumann 2015</a>)</span>. These open licences promote transparency, reproducibility, and collaboration, especially in research and public policy contexts. Conversely, commercial imagery is often constrained by licences that prohibit redistribution or require substantial payment <span class="citation" data-cites="kim2024commercial">(<a href="references.html#ref-kim2024commercial" role="doc-biblioref">Kim 2024</a>)</span>, posing barriers to open science.</p>
<p>There are also growing ethical concerns surrounding the use of satellite imagery. High-resolution images can capture detailed views of human activities and built environments, which may raise privacy issues, especially when the data are used in sensitive contexts such as humanitarian crises, armed conflict, or surveillance operations <span class="citation" data-cites="guida2021use avtar2021remote">(<a href="references.html#ref-guida2021use" role="doc-biblioref">Guida 2021</a>; <a href="references.html#ref-avtar2021remote" role="doc-biblioref">Avtar et al. 2021</a>)</span>. In addition, a broader debate has emerged around the concept of data colonialism. This term refers to the idea that access to valuable data, in this context, commercial high-resolution imagery, is often dominated by institutions in wealthier countries <span class="citation" data-cites="thatcher2016data">(<a href="references.html#ref-thatcher2016data" role="doc-biblioref">Thatcher, O’Sullivan, and Mahmoudi 2016</a>)</span>. As a result, organisations and researchers in lower-income regions may face significant barriers to accessing the data needed for critical decision-making, scientific research, or disaster response. These imbalances risk reinforcing existing global inequalities in knowledge production and technological capacity.</p>
</section>
<section id="cloud-based-repositories-and-big-data-challenges" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="cloud-based-repositories-and-big-data-challenges"><span class="header-section-number">3.4.4</span> Cloud-Based Repositories and Big Data Challenges</h3>
<p>As satellite imagery becomes more detailed and abundant, managing these large datasets using traditional methods is increasingly difficult. Cloud-based solutions address this by using formats like Cloud Optimised GeoTIFFs (COGs), which let users load only the parts of a file they need, improving efficiency and reducing the need to download entire images <span class="citation" data-cites="COG">(<a href="references.html#ref-COG" role="doc-biblioref">n.d.h</a>)</span>. Further supporting these large datasets, platforms like Amazon Web Services (AWS), Google Cloud, and Microsoft’s Planetary Computer now host massive archives of satellite data from missions such as Landsat and Sentinel. Tools provided by services like GEE allow users to search, analyse, and integrate this data into applications without needing their own servers.</p>
<p>Despite these advances, challenges remain. High storage and download costs can limit how much data users can afford to access. In addition, inconsistent metadata—how data is described and labelled—makes it harder to work across different systems. Most critically, access to cloud computing resources is uneven. Many researchers, especially in low-resource settings, may not have the internet connectivity or funding to take full advantage of these platforms <span class="citation" data-cites="lowndes2017our">(<a href="references.html#ref-lowndes2017our" role="doc-biblioref">Lowndes et al. 2017</a>)</span>.</p>
</section>
</section>
<section id="pre-processing-and-calibration" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="pre-processing-and-calibration"><span class="header-section-number">3.5</span> Pre-processing and Calibration</h2>
<p>Before satellite imagery can be meaningfully analysed, it undergoes a series of pre-processing steps to ensure spatial accuracy, radiometric consistency, and suitability for the intended application. The main stages include:</p>
</section>
<section id="georeferencing-and-orthorectification" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="georeferencing-and-orthorectification"><span class="header-section-number">3.6</span> Georeferencing and Orthorectification</h2>
<p>Georeferencing is the process of linking a satellite image to real locations on the Earth’s surface so that every pixel corresponds to a specific point on a map. Orthorectification goes a step further by correcting distortions in the image that occur because the satellite was not looking straight down, the ground is uneven, or the Earth is curved. These steps make sure that features such as roads, buildings, or rivers are shown in the right place and at the correct scale. Without these corrections, measurements taken from the image could be inaccurate, which would affect tasks like tracking city growth, monitoring environmental change, or planning new infrastructure <span class="citation" data-cites="jensen2009remote">(<a href="references.html#ref-jensen2009remote" role="doc-biblioref">Jensen 2009</a>)</span>.</p>
<section id="radiometric-and-atmospheric-corrections" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="radiometric-and-atmospheric-corrections"><span class="header-section-number">3.6.1</span> Radiometric and Atmospheric Corrections</h3>
<p>Over time, satellite sensors can lose some of their accuracy, and different sensors may record slightly different values for the same location. Radiometric calibration is a way of adjusting the image so that the brightness and colours more accurately represent what is really on the ground. Atmospheric correction deals with the effects of the air between the satellite and the Earth’s surface. Sunlight can be scattered or absorbed by gases, dust, smoke, or water vapour in the atmosphere, which can change the way surfaces appear in the image. These corrections help ensure that the colours and brightness in the imagery are as close as possible to reality, making the data more reliable for studying vegetation, tracking climate patterns, or mapping land use <span class="citation" data-cites="jensen2009remote campbell2011introduction">(<a href="references.html#ref-jensen2009remote" role="doc-biblioref">Jensen 2009</a>; <a href="references.html#ref-campbell2011introduction" role="doc-biblioref">Campbell and Wynne 2011</a>)</span>.</p>
</section>
<section id="cloud-masking-and-data-fusion" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="cloud-masking-and-data-fusion"><span class="header-section-number">3.6.2</span> Cloud Masking and Data Fusion</h3>
<p>Clouds and the shadows they cast can block important details in satellite images, making it difficult to see the land or water underneath. To address this, automated methods such as the Fmask system <span class="citation" data-cites="zhu2015improvement">(<a href="references.html#ref-zhu2015improvement" role="doc-biblioref">Zhu, Wang, and Woodcock 2015</a>)</span> can scan the image to find and remove the parts affected by clouds or their shadows. Landsat 8, for instance, also includes a dedicated Quality Assessment (QA) band that flags pixels affected by clouds, cloud shadows, snow, and other anomalies, which can be used to improve cloud masking <span class="citation" data-cites="missions2019landsat">(<a href="references.html#ref-missions2019landsat" role="doc-biblioref">Missions 2019</a>)</span>.</p>
<p>Another approach, called data fusion, combines information from different sources to fill in the gaps. For example, optical images (which rely on sunlight) can be merged with radar images (which can see through clouds), or multiple images taken on different days can be blended together. These techniques not only reduce the impact of clouds but can also make the images sharper, add more colour detail, or show changes over shorter time periods <span class="citation" data-cites="pohl1998review">(<a href="references.html#ref-pohl1998review" role="doc-biblioref">Pohl and Van Genderen 1998</a>)</span>.</p>
</section>
<section id="handling-noise-and-inconsistencies" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="handling-noise-and-inconsistencies"><span class="header-section-number">3.6.3</span> Handling Noise and Inconsistencies</h3>
<p>Satellite images can sometimes contain unwanted errors or ‘noise’ that make them harder to interpret. This noise might come from the sensor itself, interference from the atmosphere, or differences between images taken by different satellites or at different times. To improve image quality, specialists use various techniques to reduce this noise and correct inconsistencies. For example, filters can smooth out random speckles in radar images, and adjustments can be made to align images taken under different conditions. These steps help make the data clearer and more reliable for analysis <span class="citation" data-cites="maity2015comparative idol2017radar">(<a href="references.html#ref-maity2015comparative" role="doc-biblioref">Maity et al. 2015</a>; <a href="references.html#ref-idol2017radar" role="doc-biblioref">Idol, Haack, and Mahabir 2017</a>)</span>.</p>
</section>
<section id="derivation-of-indices" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4" class="anchored" data-anchor-id="derivation-of-indices"><span class="header-section-number">3.6.4</span> Derivation of Indices</h3>
<p>Satellite images capture different types of light reflected from the Earth’s surface. By combining these light measurements in special ways called “indices,” it becomes easier to see and understand certain features. One common example is the Normalised Difference Vegetation Index (NDVI), which uses light reflected from plants to show how healthy the vegetation is. Healthy plants reflect more near-infrared light and less red light, so NDVI highlights areas with thriving greenery <span class="citation" data-cites="huete2002overview">(<a href="references.html#ref-huete2002overview" role="doc-biblioref">Huete et al. 2002</a>)</span>. Other indices help detect water, assess fire damage, or identify urban areas. These tools turn complex satellite data into simple, meaningful pictures that support environmental monitoring and decision-making.</p>
</section>
</section>
<section id="analytical-methods-and-tools" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="analytical-methods-and-tools"><span class="header-section-number">3.7</span> Analytical Methods and Tools</h2>
<p>Once satellite images are prepared, different techniques are used to extract useful information.</p>
<section id="image-classification" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="image-classification"><span class="header-section-number">3.7.1</span> Image Classification</h3>
<p>Image classification is a way to sort parts of a satellite image into meaningful groups, such as forests, water bodies, or urban areas. Some techniques rely on examples provided by experts to “teach” the computer what each category looks like (supervised classification), while others automatically find patterns without prior examples (unsupervised classification) <span class="citation" data-cites="lu2007survey">(<a href="references.html#ref-lu2007survey" role="doc-biblioref">Lu and Weng 2007</a>)</span>. More recently, advanced artificial intelligence methods, like deep learning, have been used to improve accuracy by recognising complex patterns in high-resolution images <span class="citation" data-cites="li2018deep">(<a href="references.html#ref-li2018deep" role="doc-biblioref">Li et al. 2018</a>)</span>. Figure 1 shows a recent 2024 land cover map for the UK that was derived from satellite imagery.</p>
<p><img src="images/land_cover_UK_2024.png" id="fig:land_cover" class="img-fluid" style="width:80.0%" data-fig-cap="Satellite derived land cover map for UK, 2024. Source: UK Centre for Ecology and Hydrology (https://www.ceh.ac.uk/data/ukceh-land-cover-maps)"></p>
</section>
<section id="change-detection" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="change-detection"><span class="header-section-number">3.7.2</span> Change Detection</h3>
<p>Change detection involves comparing images taken at different times to identify how the landscape has changed. This is useful for monitoring deforestation, urban growth, flood damage, or other environmental changes. Techniques range from visual comparison to GIS-based approaches <span class="citation" data-cites="lu2004change">(<a href="references.html#ref-lu2004change" role="doc-biblioref">Lu et al. 2004</a>)</span>, to more recent AI and deep learning methods <span class="citation" data-cites="ding2025survey">(<a href="references.html#ref-ding2025survey" role="doc-biblioref">Ding et al. 2025</a>)</span>. By spotting where and when changes happen, decision-makers can respond more quickly to issues or plan future developments.</p>
</section>
<section id="object-based-image-analysis" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3" class="anchored" data-anchor-id="object-based-image-analysis"><span class="header-section-number">3.7.3</span> Object-Based Image Analysis</h3>
<p>Unlike methods that classify individual pixels, Object-Based Image Analysis (OBIA) groups nearby pixels into meaningful “objects” based on their shape, colour, and texture <span class="citation" data-cites="blaschke2010object">(<a href="references.html#ref-blaschke2010object" role="doc-biblioref">Blaschke 2010</a>)</span>. This approach is especially effective for identifying distinct features such as buildings, roads, or agricultural fields, providing more detailed and accurate results in complex environments.</p>
</section>
<section id="time-series-analysis-and-spatio-temporal-modelling" class="level3" data-number="3.7.4">
<h3 data-number="3.7.4" class="anchored" data-anchor-id="time-series-analysis-and-spatio-temporal-modelling"><span class="header-section-number">3.7.4</span> Time-Series Analysis and Spatio-Temporal Modelling</h3>
<p>Time-series analysis examines satellite data collected over multiple dates to observe trends and patterns over time, such as seasonal vegetation cycles or urban expansion <span class="citation" data-cites="zhang2003monitoring">(<a href="references.html#ref-zhang2003monitoring" role="doc-biblioref">Zhang et al. 2003</a>)</span>. Spatio-temporal modelling adds the dimension of space and time together to better understand how changes occur across different locations and periods.</p>
</section>
<section id="software-and-programming-tools" class="level3" data-number="3.7.5">
<h3 data-number="3.7.5" class="anchored" data-anchor-id="software-and-programming-tools"><span class="header-section-number">3.7.5</span> Software and Programming Tools</h3>
<p>Specialised software and programming languages help experts manage, analyse, and visualise satellite data. Tools like QGIS, ENVI, and SNAP provide user-friendly interfaces for working with geospatial data, while programming languages such as Python enable custom analyses through libraries like <code>rasterio</code> <span class="citation" data-cites="rasterio">(<a href="references.html#ref-rasterio" role="doc-biblioref">Rasterio Developers 2024</a>)</span> and <code>scikit-learn</code> <span class="citation" data-cites="scikit-learn">(<a href="references.html#ref-scikit-learn" role="doc-biblioref">Pedregosa et al. 2011</a>)</span>. Cloud platforms, including Google Earth Engine (GEE), have further expanded access by allowing large-scale processing without needing powerful local computers <span class="citation" data-cites="gorelick2017google">(<a href="references.html#ref-gorelick2017google" role="doc-biblioref">Gorelick et al. 2017</a>)</span>.</p>
</section>
</section>
<section id="summary" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="summary"><span class="header-section-number">3.8</span> Summary</h2>
<p>Satellite imagery offers a powerful way to observe and understand our planet from above, providing valuable information about the environment, cities, and natural resources. This chapter explored how these images are collected from both public and commercial sources, and the steps needed to prepare them for meaningful analysis. Ensuring accuracy through processes like correcting distortions and removing clouds is essential before the data can be used effectively. By applying various methods to classify land types, detect changes over time, and study patterns, satellite images become a vital tool for tracking environmental health, managing urban growth, and responding to natural disasters.</p>
<p>Beyond the technical details, the use of satellite data holds great promise for addressing global challenges. It enables better decision-making by offering a clear, up-to-date picture of complex landscapes at local and global scales. Cloud-based platforms and new technologies have made this information more accessible to a wide range of users, from scientists to policymakers. However, challenges remain, such as ensuring fair access to high-resolution data and respecting ethical considerations. Overall, satellite imagery represents an important resource that can help society monitor change, protect ecosystems, and plan more sustainable futures.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-USGS" class="csl-entry" role="listitem">
n.d.b. <em>USGS</em>. <a href="https://www.usgs.gov/landsat-missions/landsat-1">https://www.usgs.gov/landsat-missions/landsat-1</a>.
</div>
<div id="ref-ESA" class="csl-entry" role="listitem">
———. n.d.c. <em>ESA</em>. <a href="https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-1">https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-1</a>.
</div>
<div id="ref-Planet" class="csl-entry" role="listitem">
———. n.d.d. <em>Planet Labs</em>. <a href="https://www.planet.com/products/satellite-monitoring/">https://www.planet.com/products/satellite-monitoring/</a>.
</div>
<div id="ref-WorldView" class="csl-entry" role="listitem">
———. n.d.a. <em>WorldView</em>. <a href="https://earth.esa.int/eogateway/missions/worldview-3">https://earth.esa.int/eogateway/missions/worldview-3</a>.
</div>
<div id="ref-csda" class="csl-entry" role="listitem">
———. n.d.e. <em>NASA</em>. <a href="https://www.earthdata.nasa.gov/about/csda">https://www.earthdata.nasa.gov/about/csda</a>.
</div>
<div id="ref-esa_third_party" class="csl-entry" role="listitem">
———. n.d.f. <em>ESA</em>. <a href="https://earth.esa.int/eogateway/missions/third-party-missions">https://earth.esa.int/eogateway/missions/third-party-missions</a>.
</div>
<div id="ref-Copernicus" class="csl-entry" role="listitem">
———. n.d.g. <em>Copernicus</em>. <a href="https://www.copernicus.eu/en/access-data/conventional-data-access-hubs">https://www.copernicus.eu/en/access-data/conventional-data-access-hubs</a>.
</div>
<div id="ref-COG" class="csl-entry" role="listitem">
———. n.d.h. <em>Cloud Optimized GeoTIFF</em>. <a href="https://cogeo.org/">https://cogeo.org/</a>.
</div>
<div id="ref-avtar2021remote" class="csl-entry" role="listitem">
Avtar, Ram, Asma Kouser, Ashwani Kumar, Deepak Singh, Prakhar Misra, Ankita Gupta, Ali P Yunus, et al. 2021. <span>“Remote Sensing for International Peace and Security: Its Role and Implications.”</span> <em>Remote Sensing</em> 13 (3): 439.
</div>
<div id="ref-belward2015launched" class="csl-entry" role="listitem">
Belward, Alan S, and Jon O Skøien. 2015. <span>“Who Launched What, When and Why; Trends in Global Land-Cover Observation Capacity from Civilian Earth Observation Satellites.”</span> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 103: 115–28.
</div>
<div id="ref-blaschke2010object" class="csl-entry" role="listitem">
Blaschke, Thomas. 2010. <span>“Object Based Image Analysis for Remote Sensing.”</span> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 65 (1): 2–16.
</div>
<div id="ref-campbell2011introduction" class="csl-entry" role="listitem">
Campbell, James B, and Randolph H Wynne. 2011. <em>Introduction to Remote Sensing</em>. Guilford press.
</div>
<div id="ref-curzi2020large" class="csl-entry" role="listitem">
Curzi, Giacomo, Dario Modenini, and Paolo Tortora. 2020. <span>“Large Constellations of Small Satellites: A Survey of Near Future Challenges and Missions.”</span> <em>Aerospace</em> 7 (9): 133.
</div>
<div id="ref-ding2025survey" class="csl-entry" role="listitem">
Ding, Lei, Danfeng Hong, Maofan Zhao, Hongruixuan Chen, Chenyu Li, Jie Deng, Naoto Yokoya, Lorenzo Bruzzone, and Jocelyn Chanussot. 2025. <span>“A Survey of Sample-Efficient Deep Learning for Change Detection in Remote Sensing: Tasks, Strategies, and Challenges.”</span> <em>IEEE Geoscience and Remote Sensing Magazine</em>.
</div>
<div id="ref-drusch2012sentinel" class="csl-entry" role="listitem">
Drusch, Matthias, Umberto Del Bello, Sébastien Carlier, Olivier Colin, Veronica Fernandez, Ferran Gascon, Bianca Hoersch, et al. 2012. <span>“Sentinel-2: ESA’s Optical High-Resolution Mission for GMES Operational Services.”</span> <em>Remote Sensing of Environment</em> 120: 25–36.
</div>
<div id="ref-ferretti2002permanent" class="csl-entry" role="listitem">
Ferretti, Alessandro, Claudio Prati, and Fabio Rocca. 2002. <span>“Permanent Scatterers in SAR Interferometry.”</span> <em>IEEE Transactions on Geoscience and Remote Sensing</em> 39 (1): 8–20.
</div>
<div id="ref-goetz2009three" class="csl-entry" role="listitem">
Goetz, Alexander FH. 2009. <span>“Three Decades of Hyperspectral Remote Sensing of the Earth: A Personal View.”</span> <em>Remote Sensing of Environment</em> 113: S5–16.
</div>
<div id="ref-gorelick2017google" class="csl-entry" role="listitem">
Gorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. <span>“Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone.”</span> <em>Remote Sensing of Environment</em> 202: 18–27.
</div>
<div id="ref-guida2021use" class="csl-entry" role="listitem">
Guida, Emilio. 2021. <span>“The Use of Satellites in Humanitarian Contexts.”</span>
</div>
<div id="ref-harris2015open" class="csl-entry" role="listitem">
Harris, Ray, and Ingo Baumann. 2015. <span>“Open Data Policies and Satellite Earth Observation.”</span> <em>Space Policy</em> 32: 44–53.
</div>
<div id="ref-huete2002overview" class="csl-entry" role="listitem">
Huete, Alfredo, Kamel Didan, Tomoaki Miura, E Patricia Rodriguez, Xiang Gao, and Laerte G Ferreira. 2002. <span>“Overview of the Radiometric and Biophysical Performance of the MODIS Vegetation Indices.”</span> <em>Remote Sensing of Environment</em> 83 (1-2): 195–213.
</div>
<div id="ref-idol2017radar" class="csl-entry" role="listitem">
Idol, Terry, Barry Haack, and Ron Mahabir. 2017. <span>“Radar Speckle Reduction and Derived Texture Measures for Land Cover/Use Classification: A Case Study.”</span> <em>Geocarto International</em> 32 (1): 18–29.
</div>
<div id="ref-jensen2009remote" class="csl-entry" role="listitem">
Jensen, John R. 2009. <em>Remote Sensing of the Environment: An Earth Resource Perspective 2/e</em>. Pearson Education India.
</div>
<div id="ref-kim2024commercial" class="csl-entry" role="listitem">
Kim, Young-Ju. 2024. <span>“Commercial Use of Satellite Remote Sensing Data and Civil Liability.”</span> <em>Laws</em> 13 (6): 77.
</div>
<div id="ref-li2018deep" class="csl-entry" role="listitem">
Li, Ying, Haokui Zhang, Xizhe Xue, Yenan Jiang, and Qiang Shen. 2018. <span>“Deep Learning for Remote Sensing Image Classification: A Survey.”</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em> 8 (6): e1264.
</div>
<div id="ref-lowndes2017our" class="csl-entry" role="listitem">
Lowndes, Julia S Stewart, Benjamin D Best, Courtney Scarborough, Jamie C Afflerbach, Melanie R Frazier, Casey C O’hara, Ning Jiang, and Benjamin S Halpern. 2017. <span>“Our Path to Better Science in Less Time Using Open Data Science Tools.”</span> <em>Nature Ecology &amp; Evolution</em> 1 (6): 0160.
</div>
<div id="ref-lu2004change" class="csl-entry" role="listitem">
Lu, Dengsheng, Paul Mausel, Eduardo Brondizio, and Emilio Moran. 2004. <span>“Change Detection Techniques.”</span> <em>International Journal of Remote Sensing</em> 25 (12): 2365–2401.
</div>
<div id="ref-lu2007survey" class="csl-entry" role="listitem">
Lu, Dengsheng, and Qihao Weng. 2007. <span>“A Survey of Image Classification Methods and Techniques for Improving Classification Performance.”</span> <em>International Journal of Remote Sensing</em> 28 (5): 823–70.
</div>
<div id="ref-maity2015comparative" class="csl-entry" role="listitem">
Maity, Alenrex, Anshuman Pattanaik, Santwana Sagnika, and Santosh Pani. 2015. <span>“A Comparative Study on Approaches to Speckle Noise Reduction in Images.”</span> In <em>2015 International Conference on Computational Intelligence and Networks</em>, 148–55. IEEE.
</div>
<div id="ref-missions2019landsat" class="csl-entry" role="listitem">
Missions, Landsat. 2019. <span>“Landsat 8 Data Users Handbook.”</span> <em>Landsat MISSIONS: Sioux Falls, SD, USA</em>.
</div>
<div id="ref-mutanga2019google" class="csl-entry" role="listitem">
Mutanga, Onisimo, and Lalit Kumar. 2019. <span>“Google Earth Engine Applications.”</span> <em>Remote Sensing</em>. MDPI.
</div>
<div id="ref-scikit-learn" class="csl-entry" role="listitem">
Pedregosa et al. 2011. <em>Scikit-Learn: Machine Learning in Python</em>. <em>Journal of Machine Learning Research</em>. Vol. 12. <a href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</a>.
</div>
<div id="ref-pohl1998review" class="csl-entry" role="listitem">
Pohl, Cle, and John L Van Genderen. 1998. <span>“Review Article Multisensor Image Fusion in Remote Sensing: Concepts, Methods and Applications.”</span> <em>International Journal of Remote Sensing</em> 19 (5): 823–54.
</div>
<div id="ref-rasterio" class="csl-entry" role="listitem">
Rasterio Developers. 2024. <em>Rasterio: Access to Geospatial Raster Data in Python</em>. <a href="https://rasterio.readthedocs.io/en/latest/">https://rasterio.readthedocs.io/en/latest/</a>.
</div>
<div id="ref-richards2022remote" class="csl-entry" role="listitem">
Richards, John A, John A Richards, et al. 2022. <em>Remote Sensing Digital Image Analysis</em>. Vol. 5. Springer.
</div>
<div id="ref-roy2014landsat" class="csl-entry" role="listitem">
Roy, David P, Michael A Wulder, Thomas R Loveland, Curtis E Woodcock, Richard G Allen, Martha C Anderson, Dennis Helder, et al. 2014. <span>“Landsat-8: Science and Product Vision for Terrestrial Global Change Research.”</span> <em>Remote Sensing of Environment</em> 145: 154–72.
</div>
<div id="ref-EarthExplorer" class="csl-entry" role="listitem">
Survey, USGS - U. S. Geological. n.d. <em>EarthExplorer</em>. <a href="https://earthexplorer.usgs.gov/">https://earthexplorer.usgs.gov/</a>.
</div>
<div id="ref-thatcher2016data" class="csl-entry" role="listitem">
Thatcher, Jim, David O’Sullivan, and Dillon Mahmoudi. 2016. <span>“Data Colonialism Through Accumulation by Dispossession: New Metaphors for Daily Data.”</span> <em>Environment and Planning D: Society and Space</em> 34 (6): 990–1006.
</div>
<div id="ref-wulder2019current" class="csl-entry" role="listitem">
Wulder, Michael A, Thomas R Loveland, David P Roy, Christopher J Crawford, Jeffrey G Masek, Curtis E Woodcock, Richard G Allen, et al. 2019. <span>“Current Status of Landsat Program, Science, and Applications.”</span> <em>Remote Sensing of Environment</em> 225: 127–47.
</div>
<div id="ref-wulder2012opening" class="csl-entry" role="listitem">
Wulder, Michael A, Jeffrey G Masek, Warren B Cohen, Thomas R Loveland, and Curtis E Woodcock. 2012. <span>“Opening the Archive: How Free Data Has Enabled the Science and Monitoring Promise of Landsat.”</span> <em>Remote Sensing of Environment</em> 122: 2–10.
</div>
<div id="ref-zhang2003monitoring" class="csl-entry" role="listitem">
Zhang, Xiaoyang, Mark A Friedl, Crystal B Schaaf, Alan H Strahler, John CF Hodges, Feng Gao, Bradley C Reed, and Alfredo Huete. 2003. <span>“Monitoring Vegetation Phenology Using MODIS.”</span> <em>Remote Sensing of Environment</em> 84 (3): 471–75.
</div>
<div id="ref-zhao2021progress" class="csl-entry" role="listitem">
Zhao, Qiang, Le Yu, Xuecao Li, Dailiang Peng, Yongguang Zhang, and Peng Gong. 2021. <span>“Progress and Trends in the Application of Google Earth and Google Earth Engine.”</span> <em>Remote Sensing</em> 13 (18): 3778.
</div>
<div id="ref-zhu2015improvement" class="csl-entry" role="listitem">
Zhu, Zhe, Shixiong Wang, and Curtis E Woodcock. 2015. <span>“Improvement and Expansion of the Fmask Algorithm: Cloud, Cloud Shadow, and Snow Detection for Landsats 4–7, 8, and Sentinel 2 Images.”</span> <em>Remote Sensing of Environment</em> 159: 269–77.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Sputnik_1">Sputnik 1</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://en.wikipedia.org/wiki/Landsat_1">Landsat 1</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/intro.html" class="pagination-link" aria-label="Introduction: Why Imagery, Why Now?">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction: Why Imagery, Why Now?</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/01_use-case-x.html" class="pagination-link" aria-label="Use Case 1">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Use Case 1</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Imago | UKRI Imagery Data Service</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/Imago-SDRUK/boi/edit/main/chapters/image-data.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/Imago-SDRUK/boi/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>